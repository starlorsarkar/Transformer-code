{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2355cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math as math\n",
    "import torch.nn as nn\n",
    "class inputembedding(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  #length of each vector \n",
    "        self.embedding = nn.embedding(d_model,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)* math.sqrt(d_model) # tensor of size and dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476fe33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model \n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "# create a matrix of shape\n",
    "        pe = torch.zeros(seq_len,d_model)\n",
    "# create vector of shapes \n",
    "        po = torch.arrange(0,seq_len,dtype=torch.float).unsequence(1)\n",
    "        \n",
    "        # cos \n",
    "        \n",
    "        div_term = torch.exp(torch.arranage(0,d_model,2,dtype=float))*(-math.log(10000.0)/d_model)\n",
    "        \n",
    "        #apply sin \n",
    "        \n",
    "        pe[:,0::2]= torch.sin(po*div_term)\n",
    "        pe[:,1::2] = torch.cos(po*div_term)\n",
    "        \n",
    "        pe= pe.unsqueeze(0)\n",
    "        self.buffers('pe',pe)\n",
    "        \n",
    "        def forward(self,x):\n",
    "            x = x+(self.pe[:, :x.shape(1),:]).requires_grad_(False)\n",
    "            return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acac87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layernorm(nn.Module):\n",
    "    def __init__(self,eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.parameter(torch.ones(1)) #multiply \n",
    "        self.bias = nn.parameter(torch.zeros(1)) \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        std = x.std(dim=-1,keepdim=True)\n",
    "        return self.alpha* (x-mean)/(std+eps)+self.bias\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a17c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforward(nn.Module):\n",
    "    \n",
    "    def __init__ (self,d_model:int,d_diff:int,dropout: float):\n",
    "        super.__init__ ()\n",
    "        self.linear1 = nn.linear(d_model,d_diff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.linear(d_model,d_diff)\n",
    "        \n",
    "        def forward():\n",
    "            x = self.linear1(x)\n",
    "            x = self.relu(x)\n",
    "            x= self.dropout(x)\n",
    "            x = self.linear2(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ffdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadattention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model:int, num_heads:int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0 , \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads # dk is dimension of each head\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model)\n",
    "        self.w_v = nn.Linear(d_model,d_model)\n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        @staticmethod\n",
    "        \n",
    "        def attention(query,key,value,mask=None,dropout=None):\n",
    "            d_k = query.shape[-1]\n",
    "            attention_scores = (query.matmul(key.transpose(-2,-1)))/math.sqrt(d_k)\n",
    "            if mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(mask==0,-1e9)\n",
    "                attention_score  = attention_scores.softmax(dim=-1) # batch size x num_heads x seq len x seq len\n",
    "                if dropout is not None:\n",
    "                    attention_score = dropout(attention_score)\n",
    "                    \n",
    "                return attention_score.matmul(value), attention_score\n",
    "        \n",
    "        \n",
    "        def forward(self,query,key,value,mask=None):\n",
    "            query = self.w_q(query) # batch size x seq len x d_model\n",
    "            key = self.w_k(key)\n",
    "            value = self.w_v(value)\n",
    "            # (bathch size x seq len x num_heads x d_k ) -- ( batch size x num_heads x seq len x d_k)     \n",
    "            query =query.view(query.size[0], query.size[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "            key = key.view(key.size[0], key.size[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "            value = value.view(value.size[0], value.size[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "            \n",
    "        x,self.attention_score = multiheadattention.attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "        # batch size x num_heads x seq len x d_k -- batch size x seq len x num_heads x d_k --- batch size x seq len x d_model\n",
    "        x = x.transpose(1,2).contiguous().view(x.size[0],-1,self.num_heads*self.d_k)\n",
    "        # batch size x seq len x d_model - batch size x seq len x d_model\n",
    "        return self.w_o(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe75d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual(nn.Module):\n",
    "    def __init__ (self, dropout : float):\n",
    "        super(). __init__ ()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = layernorm(d_model)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        return x+ self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderblock(nn.Module):\n",
    "    def __init__(self,self_attenion: multiheadattention, cross_attention: multiheadattention,feedforward: feedforward, residual: residual, d_model:int, num_heads:int, d_diff:int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attenion\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feedforward = feedforward\n",
    "        self.residual = nn.ModuleList([residual(dropout) for _ in range(3)])\n",
    "        \n",
    "    def forward(self,x,encoder_output,src_mask,tgt_mask): # soource mask is coming from encoder and target mask is coming from decoder\n",
    "        x = self.residual[0](x, lambda x: self.self_attention(x,x,x,tgt_mask))\n",
    "        x = self.residual[1](x, lambda x: self.cross_attention(x,encoder_output,encoder_output,src_mask))\n",
    "        x = self.residual[2](x, self.feedforward)\n",
    "        return x\n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__ (self,layers: nn.ModuleList, norm: layernorm):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = norm\n",
    "        \n",
    "    def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,src_mask,tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3df4048",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2297516494.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 32\u001b[1;36m\u001b[0m\n\u001b[1;33m    return self.projection(x):\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class projection(nn.Module):\n",
    "    def __init__ (self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.log_softmax(self.linear(x),dim=-1)\n",
    "    \n",
    "class transformer(nn.Module):\n",
    "    def __init__ (self, encoder:encoder ,decoder:decoder,src_embedding: inputembedding, tgt_embedding: inputembedding,src_positional_embedding: positional_embedding, tgt_positional_embedding: positional_embedding, projection: projection):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.src_embedding = src_embedding\n",
    "        self.tgt_embedding = tgt_embedding\n",
    "        self.src_positional_embedding = src_positional_embedding\n",
    "        self.tgt_positional_embedding = tgt_positional_embedding\n",
    "        self.projection = projection\n",
    "        \n",
    "    def encode(self,src,src_mask):\n",
    "        src = self.src_embedding(src)\n",
    "        src = self.src_positional_embedding(src)\n",
    "        return self.encoder(src,src_mask)\n",
    "    \n",
    "    def decode(self,encoder_output,src_mask,tgt,tgt_mask):\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        tgt = self.tgt_positional_embedding(tgt)\n",
    "        return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
    "    \n",
    "    \n",
    "    def project(self,x):\n",
    "        return self.projection(x):\n",
    "    \n",
    "    \n",
    "    def build_transformers(src_vocab_size:int, tgt_vocab_size:int, seq_len:int, d_model:int=512, num_heads:int=8, d_diff:int=2048, num_layers:int=6, dropout: float=0.1):\n",
    "        \n",
    "        # create the embedding layers \n",
    "        src_embedding = inputembedding(d_model,src_vocab_size)\n",
    "        tgt_embedding = inputembedding(d_model,tgt_vocab_size)\n",
    "        \n",
    "        \n",
    "        src_positional_embedding = positional_embedding(d_model,seq_len,dropout)\n",
    "        tgt_positional_embedding = positional_embedding(d_model,seq_len,dropout)\n",
    "        \n",
    "        # create the an encoder blocks\n",
    "        encoder_block = []\n",
    "        for _ in range(num_layers):\n",
    "            e_self_attention = multiheadattention(d_model,num_heads,dropout)\n",
    "            e_feedforward_network = feedforward(d_model,d_diff,dropout)\n",
    "            e_residual_connection = residual(dropout)\n",
    "            encoder_layer = encoderblock(e_self_attention,e_feedforward_network,e_residual_connection,d_model,num_heads,d_diff,dropout)\n",
    "            encoder_block.append(encoder_layer)\n",
    "            \n",
    "            \n",
    "        # create the decoder blocks \n",
    "        decoder_block = []\n",
    "        for _ in range(num_layers):\n",
    "            d_self_attention = multiheadattention(d_model,num_heads,dropout)\n",
    "            d_cross_attention = multiheadattention(d_model,num_heads,dropout)\n",
    "            d_feedforward_network = feedforward(d_model,d_diff,dropout)\n",
    "            d_residual_connection = residual(dropout)\n",
    "            decoder_layer = decoderblock(d_self_attention,d_cross_attention,d_feedforward_network,d_residual_connection,d_model,num_heads,d_diff,dropout)\n",
    "            decoder_block.append(decoder_layer)\n",
    "    # create encoder and decoder \n",
    "    encoder = encoder(nn.ModuleList(encoder_block),layernorm(d_model))\n",
    "    decoder = decoder(nn.ModuleList(decoder_block),layernorm(d_model))\n",
    "    # create projection layer\n",
    "    projection_layer = projection(d_model,tgt_vocab_size)\n",
    "    \n",
    "    # create the transformer\n",
    "    transformer_model = transformer(encoder,decoder,src_embedding,tgt_embedding,src_positional_embedding,tgt_positional_embedding,projection_layer)\n",
    "    \n",
    "    #initialize the parameters \n",
    "    \n",
    "    for p in transformer_model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "            \n",
    "            return transformer_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
